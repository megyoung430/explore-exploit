import numpy as np
import pandas as pd
from database_fxns import *
from analysis_fxns import *
import torch

"""
TO DO: check on synthetic data and then check with all the animals

Write this in pytorch to be able to take the gradients

Old Code

def getLikelihood(session, alpha, beta, learning_rate):

    def getPolicy(previous_action):
        
        stickiness = np.zeros(len(q_values))
        if previous_action is not None:
            stickiness[previous_action] += alpha
        
        exponent = np.exp(beta * (q_values + stickiness))
        probabilities = exponent / np.sum(exponent)
        return probabilities
    
    def updateQvalues(q_values, action, reward, learning_rate):
        q_values[action] += learning_rate*(reward - q_values[action])
        return q_values
    
    actions, rewards = getActionsandRewards(session)
    log_likelihood = 0
    previous_action = None
    q_values = np.zeros(3)

    for i in range(len(actions)):
        curr_probabilities = getPolicy(previous_action)
        log_likelihood += np.log(curr_probabilities[actions[i]])
        q_values = updateQvalues(q_values, actions[i], rewards[i], learning_rate)
        previous_action = actions[i]
    
    return log_likelihood

"""

def getLikelihood(session, alpha, beta, learning_rate):
    def get_policy(previous_action, q_values):
        stickiness = torch.zeros(len(q_values))
        if previous_action is not None:
            stickiness[previous_action] += alpha
        
        exponent = torch.exp(beta * (q_values + stickiness))
        probabilities = exponent / torch.sum(exponent)
        return probabilities
    
    def update_q_values(q_values, action, reward, learning_rate):
        q_values[action] += learning_rate * (reward - q_values[action])
        return q_values
    
    actions, rewards = getActionsandRewards(session)
    log_likelihood = 0
    previous_action = None
    q_values = torch.zeros(3, requires_grad=False)

    for action, reward in zip(actions, rewards):
        curr_probabilities = get_policy(previous_action, q_values)
        log_likelihood += torch.log(curr_probabilities[action])
        q_values = update_q_values(q_values, action, reward, learning_rate)
        previous_action = action
    
    return log_likelihood

alpha = torch.tensor(4.0, requires_grad=True)
beta = torch.tensor(10.0, requires_grad=True)
learning_rate = 0.05

optimizer = torch.optim.SGD([alpha, beta], lr=0.0005)

mouse = "MY_61C"
dates_of_interest = ["2024-04-10", "2024-04-11", "2024-04-12", "2024-04-13", "2024-04-14", "2024-04-15", "2024-04-16", "2024-04-18", "2024-04-19", "2024-04-20", "2024-04-21"]
sessions = getDataforSomeSessions(mouse, dates_of_interest)
preprocessed_sessions = []
for session in sessions:
    preprocessed_session = getPreprocessedSessionData(session)
    preprocessed_sessions.append(preprocessed_session)

total_epochs = 1000
num_epochs = 0
improvement = 1
previous_loss = 0
criterion = 0.00001

while num_epochs < total_epochs and np.abs(improvement) > criterion:
    optimizer.zero_grad()
    overall_likelihood = 0
    for session in preprocessed_sessions:
        curr_likelihood = getLikelihood(session, alpha, beta, learning_rate)
        overall_likelihood += curr_likelihood
    loss = -1*overall_likelihood
    loss.backward()
    optimizer.step()
    improvement = loss.item() - previous_loss
    previous_loss = loss.item()
    print(f"Epoch {num_epochs+1}, Loss: {loss.item()}")
    num_epochs = num_epochs + 1

optimized_alpha = alpha.item()
optimized_beta = beta.item()

print("Optimized Alpha:", optimized_alpha)
print("Optimized Beta:", optimized_beta)