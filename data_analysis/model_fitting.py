import sys
import os

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from database_fxns import *
from analysis_fxns import *
from simulations.bandit_task_simulation import *
import torch

"""
TO DO: check on synthetic data and then check with all the animals

Write this in pytorch to be able to take the gradients

Old Code

def getLikelihood(session, alpha, beta, learning_rate):

    def getPolicy(previous_action):
        
        stickiness = np.zeros(len(q_values))
        if previous_action is not None:
            stickiness[previous_action] += alpha
        
        exponent = np.exp(beta * (q_values + stickiness))
        probabilities = exponent / np.sum(exponent)
        return probabilities
    
    def updateQvalues(q_values, action, reward, learning_rate):
        q_values[action] += learning_rate*(reward - q_values[action])
        return q_values
    
    actions, rewards = getActionsandRewards(session)
    log_likelihood = 0
    previous_action = None
    q_values = np.zeros(3)

    for i in range(len(actions)):
        curr_probabilities = getPolicy(previous_action)
        log_likelihood += np.log(curr_probabilities[actions[i]])
        q_values = updateQvalues(q_values, actions[i], rewards[i], learning_rate)
        previous_action = actions[i]
    
    return log_likelihood

"""

"""
def getLikelihood(actions, rewards, alpha, beta, learning_rate):
    def get_policy(previous_action, q_values):
        stickiness = torch.zeros(len(q_values))
        if previous_action is not None:
            stickiness[previous_action] += alpha
        
        exponent = torch.exp(beta * (q_values + stickiness))
        probabilities = exponent / torch.sum(exponent)
        return probabilities
    
    def update_q_values(q_values, action, reward, learning_rate):
        q_values[action] += learning_rate * (reward - q_values[action])
        return q_values
    
    log_likelihood = 0
    previous_action = None
    q_values = torch.zeros(3, requires_grad=False)

    for action, reward in zip(actions, rewards):
        curr_probabilities = get_policy(previous_action, q_values)
        log_likelihood += torch.log(curr_probabilities[action])
        q_values = update_q_values(q_values, action, reward, learning_rate)
        previous_action = action
    
    return log_likelihood

def getParameterSweep(true_alpha, true_beta, alphas, betas, plot=True):

    log_likelihoods = np.zeros(shape=(len(alphas), len(betas)))
    
    for i in range(len(alphas)):
        for j in range(len(betas)):
            
            # Define the parameters and structure of the task
            num_trials = 1000
            arms_across_states = [[0.5,0.9,0.1],[0.5,0.1,0.9]]
            num_arms = len(arms_across_states[0])
            drift_rates = np.zeros(num_arms)

            num_states = len(arms_across_states)
            state_switch_probability = 0.02
            state_change_probabilities = np.full((num_states,num_states),state_switch_probability)
            np.fill_diagonal(state_change_probabilities, 1-state_switch_probability)

            # Define the parameters for the mouse
            learning_rate = 0.05
            initial_state_prior = np.full((num_states),0.5)
            alpha = true_alpha
            beta = true_beta

            bandit_task = CorrelatedBanditTask(arms_across_states, state_change_probabilities)
            mouse = QLearningInferenceMouse(num_arms, num_states, initial_state_prior, learning_rate, beta, alpha)
            actions, rewards, switches, state_changes, posterior, q_states, q_actions = bandit_task.simulate(mouse, num_trials)

            log_likelihoods[i,j] = getLikelihood(actions, rewards, alphas[i], betas[j], learning_rate) * -1

            print("Alpha: " + str(alphas[i]) + " and Beta: " + str(betas[j]) + " DONE")
    
    if plot:
        alpha_indices = np.linspace(0, len(alphas) - 1, 10, dtype=int)
        beta_indices = np.linspace(0, len(betas) - 1, 10, dtype=int)

        min_val = np.min(log_likelihoods)
        max_val = np.max(log_likelihoods)
        plt.imshow(log_likelihoods, cmap='viridis', vmin=min_val, vmax=max_val)
        plt.colorbar()
        plt.xticks(beta_indices, [betas[i] for i in beta_indices])
        plt.yticks(alpha_indices, [alphas[i] for i in alpha_indices])
        plt.xlabel("Beta")
        plt.ylabel("Alpha")
        plt.title("Log Likelihoods Across Different Parameters")
        plt.show()

    return actions, rewards, log_likelihoods

# Get synthetic data
true_alpha = 0.25
true_beta = 2.5

alpha_start = -1
alpha_end = 1
alphas = np.linspace(alpha_start, alpha_end, num=20, endpoint=True).tolist()
alphas = [round(x, 2) for x in alphas]

beta_start = 10**-1
beta_end = 10**1/2
betas = np.linspace(beta_start, beta_end, num=20, endpoint=True).tolist()
betas = [round(x, 2) for x in betas]

actions, rewards, log_likelihoods = getParameterSweep(true_alpha, true_beta, alphas, betas)

# Fit the parameters

alpha = torch.tensor(0.0, requires_grad=True)
beta = torch.tensor(0.0, requires_grad=True)
learning_rate = 0.05

optimizer = torch.optim.SGD([alpha, beta], lr=0.0001)
trajectory = []

total_epochs = 5000
num_epochs = 0
improvement = 1
previous_loss = 0
criterion = 0

# while num_epochs < total_epochs and np.abs(improvement) > criterion:
while num_epochs < total_epochs:
    trajectory.append((alpha.item(), beta.item()))
    optimizer.zero_grad()
    overall_likelihood = 0
    curr_likelihood = getLikelihood(actions, rewards, alpha, beta, learning_rate)
    overall_likelihood += curr_likelihood
    loss = -1*overall_likelihood
    loss.backward()
    optimizer.step()
    improvement = loss.item() - previous_loss
    previous_loss = loss.item()
    print(f"Epoch {num_epochs+1}, Loss: {loss.item()}, Alpha: {alpha.item()}, Beta: {beta.item()}")
    num_epochs = num_epochs + 1

optimized_alpha = alpha.item()
optimized_beta = beta.item()

print("Optimized Alpha:", optimized_alpha)
print("Optimized Beta:", optimized_beta)

alpha_traj, beta_traj = zip(*trajectory)
alpha_indices = np.linspace(0, len(alphas) - 1, 10, dtype=int)
beta_indices = np.linspace(0, len(betas) - 1, 10, dtype=int)

min_val = np.min(log_likelihoods)
max_val = np.max(log_likelihoods)

plt.imshow(log_likelihoods, cmap='viridis', vmin=min_val, vmax=max_val, extent=[betas[0], betas[-1], alphas[0], alphas[-1]], aspect='auto', origin='lower')
plt.colorbar()

plt.xticks(beta_indices, [f'{betas[i]:.2f}' for i in beta_indices])
plt.yticks(alpha_indices, [f'{alphas[i]:.2f}' for i in alpha_indices])
plt.xlabel("Beta")
plt.ylabel("Alpha")
plt.title("Log Likelihoods Across Different Parameters")

alpha_coords, beta_coords = zip(*trajectory)

alpha_traj_indices = [np.interp(alpha, alphas, np.arange(len(alphas))) for alpha in alpha_coords]
beta_traj_indices = [np.interp(beta, betas, np.arange(len(betas))) for beta in beta_coords]
true_alpha_index = np.interp(true_alpha, alphas, np.arange(len(alphas)))
true_beta_index = np.interp(true_beta, betas, np.arange(len(betas)))

plt.plot(beta_traj_indices, alpha_traj_indices, color='black', marker='o', linestyle='-', linewidth=2, markersize=5)
plt.scatter(true_beta_index, true_alpha_index, color='red', marker='o', s=50)
plt.show()
"""

"""
min_val = np.min(log_likelihoods)
max_val = np.max(log_likelihoods)
plt.imshow(log_likelihoods, cmap='viridis', vmin=min_val, vmax=max_val, extent=[betas[0], betas[-1], alphas[0], alphas[-1]], aspect='auto')
plt.plot(beta_traj, alpha_traj, color='black', marker='o', linestyle='-', linewidth=2, markersize=5)
plt.plot(true_beta, true_alpha, color='red', marker='o', linewidth=2, markersize=5)
plt.colorbar()
plt.xticks(beta_indices, [betas[i] for i in beta_indices])
plt.yticks(alpha_indices, [alphas[i] for i in alpha_indices])
plt.xlabel("Beta")
plt.ylabel("Alpha")
plt.title("Log Likelihoods Across Different Parameters")
plt.show()
"""

"""
total_epochs = 1000
num_epochs = 0
improvement = 1
previous_loss = 0
criterion = 0.00001

while num_epochs < total_epochs and np.abs(improvement) > criterion:
    optimizer.zero_grad()
    overall_likelihood = 0
    for session in preprocessed_sessions:
        actions, rewards = getActionsandRewards(session)
        curr_likelihood = getLikelihood(actions, rewards, alpha, beta, learning_rate)
        overall_likelihood += curr_likelihood
    loss = -1*overall_likelihood
    loss.backward()
    optimizer.step()
    improvement = loss.item() - previous_loss
    previous_loss = loss.item()
    print(f"Epoch {num_epochs+1}, Loss: {loss.item()}")
    num_epochs = num_epochs + 1

optimized_alpha = alpha.item()
optimized_beta = beta.item()

print("Optimized Alpha:", optimized_alpha)
print("Optimized Beta:", optimized_beta)
"""