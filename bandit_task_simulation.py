import numpy as np
import random

class QLearningMouse():

    def __init__(self, num_arms, learning_rate, beta, alpha, opto_change):
        self.q_values = np.zeros(num_arms)
        self.learning_rate = learning_rate
        self.curr_opto = False
        self.beta = beta
        self.alpha = alpha
        self.opto_change = opto_change
    
    def choose_action(self, previous_action):
        
        exponent = self.q_values
        if previous_action is not None:
            exponent[previous_action] += self.alpha
        
        exponent = np.exp(self.beta * exponent)
        probabilities = exponent / np.sum(exponent)
        return np.random.choice(range(len(exponent)), p=probabilities)
    
    def update_q_values(self, action, reward):
        self.q_values[action] += self.learning_rate * (reward - self.q_values[action])

class QLearningInferenceMouse():
    
    def __init__(self, num_arms, num_states, initial_state_prior, learning_rate, beta, alpha, opto_change):
        self.q_actions = np.zeros(num_arms)
        self.q_states = np.zeros(num_states + 1)
        self.posterior = initial_state_prior

        self.learning_rate = learning_rate
        self.beta = beta
        self.alpha = alpha

        self.curr_opto = False
        self.opto_change = opto_change
    
    def choose_action(self):

        exponent = self.q_actions
        if previous_action is not None:
            exponent[previous_action] += self.alpha
        
        exponent = np.exp(self.beta * exponent)
        probabilities = exponent / np.sum(exponent)
        return np.random.choice(range(len(exponent)), p=probabilities)
    
    def infer_state(self, bandit_task, action, reward):

        """
        This function implements recursive Bayesian filtering.

        Inputs: 
            - bandit_task (CorrelatedBanditTask object): 
            - action (int, either 0, 1, or 2):
            - reward (int, either 0 or 1): 
        """

        if action == 0:
            # Since the mouse chose the uncorrelated arm, we use a flat likelihood of 0.5
            likelihood = 0.5

            # But we still update the priors as we did previously
            updated_prior_high = p_no_switch * self.state_prior[0] + p_switch * (1 - self.state_prior[0])
            updated_prior_low = p_no_switch * self.state_prior[0] + p_switch * (1 - self.state_prior[0])

            # And calculate the new posteriors
            self.posterior[0] = likelihood * updated_prior_high
            self.posterior[1] = likelihood * updated_prior_low

            # And normalize the probabilities
            self.posterior = self.posterior/np.sum(self.posterior)
        else:
            # We start by calculating the probability that arm 1 is the high arm
            # given all of our observations

            # First, we calculate the likelihood, i.e., the probability of getting a 
            # reward after having picked an action given that arm 1 is the high arm
            if reward == 1 & action == 1:
                likelihood = max(bandit_task.arms_across_states[0])
            elif reward == 0 & action == 1:
                likelihood = 1 - max(bandit_task.arms_across_states[0])
            elif reward == 1 & action == 2:
                likelihood = 1 - max(bandit_task.arms_across_states[0])
            elif reward == 0 & action == 2:
                likelihood = max(bandit_task.arms_across_states[0])

            # Then, we calculate our prior, i.e., the probability that arm 1 is the high
            # arm given our previous history of observations
            p_no_switch = bandit_task.state_change_probabilities[0,0]
            p_switch = 1 - p_no_switch
            updated_prior_high = p_no_switch * self.posterior[0] + p_switch * (1 - self.posterior[0])

            # And calculate the posterior
            self.posterior[0] = likelihood * updated_prior_high

            # Then, we repeat the same process to calculate the probability that arm 1
            # is the low arm given all of our observations
            if reward == 1 & action == 1:
                likelihood = 1 - max(bandit_task.arms_across_states[0])
            elif reward == 0 & action == 1:
                likelihood = max(bandit_task.arms_across_states[0])
            elif reward == 1 & action == 2:
                likelihood = max(bandit_task.arms_across_states[0])
            elif reward == 0 & action == 2:
                likelihood = 1 - max(bandit_task.arms_across_states[0])
            updated_prior_low = p_no_switch * self.posterior[1] + p_switch * (1 - self.posterior[1])
            self.posterior[1] = likelihood * updated_prior_low

            # Normalize the probabilities
            self.posterior = self.posterior/np.sum(self.posterior)

    def update_q_values(self, action, reward):

        # If the mouse chooses the uncorrelated arm, update its Q value as you would normally
        if action == 0:
            self.q_states[0] += self.learning_rate * (reward - self.q_states[0])
            self.q_actions[0] = self.q_states[0]
        # If the mouse chooses either of the correlated arms, we distribute the Q update according 
        # to the probability of being in each state
        elif action == 1:
            self.q_states[1] += self.learning_rate * self.posterior[0] * (reward - self.q_states[1])
            self.q_states[2] += self.learning_rate * self.posterior[1] * (reward - self.q_states[2])
            self.q_actions[1] = self.posterior[0] * self.q_states[1] + self.posterior[1] * self.q_states[2]
        # Since the posterior is defined in terms of arm 1, we switch the probabilities for the Q update for arm 2
        elif action == 2:
            self.q_states[1] += self.learning_rate * (1 - self.posterior[0]) * (reward - self.q_states[1])
            self.q_states[2] += self.learning_rate * (1 - self.posterior[1]) * (reward - self.q_states[2])
            self.q_actions[2] = (1 - self.posterior[0]) * self.q_states[1] + (1 - self.posterior[1]) * self.q_states[2]

class MultiArmedBanditTask():

    def __init__(self, arms, drift_rates, opto_probability, opto_trial_threshold, delta_beta, delta_alpha):
        self.arms = arms
        self.num_arms = len(arms)
        self.drift_rates = drift_rates
        self.opto_probability = opto_probability
        self.opto_trial_threshold = opto_trial_threshold
        self.delta_beta = delta_beta
        self.delta_alpha = delta_alpha
    
    def get_reward(self, arm):
        if random.random() < self.arms[arm]:
            return 1
        else: 
            return 0
    
    def drift_arms(self):
        for i in range(self.num_arms):
            noise = np.random.normal(0, self.drift_rates[i])
            self.arms[i] = self.arms[i] + noise
        self.arms = np.clip(self.arms, 0, 1)
    
    def begin_opto(self, agent):
        if random.random() < self.opto_probability:
            agent.curr_opto = True
            if agent.opto_change == "increase exploration":
                agent.beta *= self.delta_beta
            elif agent.opto_change == "decrease stickiness":
                agent.alpha -= self.delta_alpha
            elif agent.opto_change == "both":
                agent.beta *= self.delta_beta
                agent.alpha -= self.delta_alpha

    def end_opto(self, agent):
        agent.curr_opto = False
        if agent.opto_change == "increase exploration":
            agent.beta /= self.delta_beta
        elif agent.opto_change == "decrease stickiness":
            agent.alpha += self.delta_alpha
        elif agent.opto_change == "both":
            agent.beta /= self.delta_beta
            agent.alpha += self.delta_alpha
    
    def simulate(self, agent, num_trials):
        previous_action = None
        actions = []
        rewards = []
        switches = []

        for i in range(num_trials):
            current_action = agent.choose_action(previous_action)
            actions.append(current_action)
            if previous_action != current_action:
                switches_details = {
                    "trial_num": i,
                    "high_arm": np.argmax(self.arms),
                    "from": previous_action,
                    "to": current_action
                }
                switches.append(switches_details)
            
            reward = self.get_reward(current_action)
            rewards.append(reward)

            agent.update_q_values(current_action, reward)
            previous_action = current_action
        
        return actions, rewards, switches

class CorrelatedBanditTask(MultiArmedBanditTask):

    def __init__(self, arms_across_states, state_change_probabilities, opto_probability, opto_trial_threshold, delta_beta, delta_alpha):
        self.arms_across_states = arms_across_states
        self.state_change_probabilities = state_change_probabilities
        self.num_states = len(arms_across_states)

        self.curr_state = np.random.choice(self.num_states)
        self.curr_arms = arms_across_states[self.curr_state]
        self.num_arms = len(self.curr_arms)

        self.opto_probability = opto_probability
        self.opto_trial_threshold = opto_trial_threshold
        self.delta_beta = delta_beta
        self.delta_alpha = delta_alpha
    
    def get_reward(self, arm):
        if random.random() < self.curr_arms[arm]:
            return 1
        else: 
            return 0

    def update_states(self, trial_num):
        if random.random() < self.state_change_probabilities[self.curr_state, np.abs(self.curr_state - 1)]:
            prev_state = self.curr_state
            self.curr_state = np.abs(self.curr_state - 1)
            self.curr_arms = self.arms_across_states[self.curr_state]
            state_changes_details = {
                "trial_num": trial_num + 1,
                "from": prev_state,
                "to": self.curr_state
            }
            return state_changes_details
        else:
            return None
    
    def simulate(self, agent, num_trials):
        previous_action = None
        actions = []
        rewards = []
        switches = []
        state_changes = []
        opto_trials = []

        for i in range(num_trials):
            # The mouse picks the action
            current_action = agent.choose_action(previous_action)
            actions.append(current_action)
            if previous_action != current_action:
                switches_details = {
                    "trial_num": i,
                    "high_arm": np.argmax(self.curr_arms),
                    "from": previous_action,
                    "to": current_action
                }
                switches.append(switches_details)

            # The mouse receives a probabilistic reward
            reward = self.get_reward(current_action)
            rewards.append(reward)

            # If the mouse has underwent optogenetic stimulation, 
            # Keep track of the trial info immediately after the optogenetic stimulation
            # And then turn off the stimulation after the end of the trial
            if agent.curr_opto:
                opto_details = {
                    "trial_num": i,
                    "high_arm": np.argmax(self.curr_arms),
                    "previous_action": previous_action,
                    "current_action": current_action
                }
                opto_trials.append(opto_details)
                self.end_opto(agent)
            # If the mouse is both trained and chooses the high arm, then initiate optogenetic stimulation for the next trial
            if (num_trials > self.opto_trial_threshold) & (previous_action == np.argmax(self.curr_arms)):
                self.begin_opto(agent)

            # The mouse updates its estimates of the value of the different bandits
            agent.update_q_values(current_action, reward)

            # Update the previous action and the state for the next trial
            previous_action = current_action
            state_changes_details = self.update_states(i)
            if state_changes_details is not None:
                state_changes.append(state_changes_details)
        
        return actions, rewards, switches, state_changes, opto_trials