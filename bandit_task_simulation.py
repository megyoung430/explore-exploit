import numpy as np
import random

"""
AGENTS
"""

class QLearningMouse():

    """
    This class implements a standard Q-learning agent.
    """

    def __init__(self, num_arms, learning_rate, beta, alpha, opto_change):
        self.q_values = np.zeros(num_arms)
        self.learning_rate = learning_rate
        self.curr_opto = False
        self.beta = beta
        self.alpha = alpha
        self.opto_change = opto_change
    
    def choose_action(self, previous_action):
        
        stickiness = np.zeros(len(self.q_values))
        if previous_action is not None:
            stickiness[previous_action] += self.alpha
        
        exponent = np.exp(self.beta * (self.q_values + stickiness))
        probabilities = exponent / np.sum(exponent)
        return np.random.choice(range(len(exponent)), p=probabilities)
    
    def update_q_values(self, action, reward):
        self.q_values[action] += self.learning_rate * (reward - self.q_values[action])

class InferenceMouse():

    """
    This class implements a latent state inference agent. 
    
    Here, The agent infers the latent state after observing the outcome (reward / no reward) of its action. However, in contrast 
    to the other Q-learning agents, this agent does not store any Q-values. Instead, after updating its posterior over the latent
    states, this agent uses these probabilities of the latent state to calculate each action's expected value. Then, the agent 
    chooses its next action based on these expected values using softmax action selection.

    This agent is still relatively specific to the first iteration of the behavioral task, which has three arms:
        - Arm 0: 0.5 probability of reward, uncorrelated to the other two arms / regardless of the current state
        - Arm 1: 0.9 probability of reward in State 0 ("Arm 1 High/Arm 2 Low") and 0.1 in State 1 ("Arm 1 Low"), perfectly anti-correlated to Arm 2
        - Arm 2: 0.1 probability of reward in State 0 ("Arm 1 High/Arm 2 Low") and 0.9 in State 1 ("Arm 1 Low"/"Arm 2 High"), perfectly anti-correlated to Arm 1
    This agent also has explicit access to certain task parameters, mainly the reward probabilities for each arm.

    Parameters:
        - posterior (num_states x 1 array): the posterior distribution over states
        - expectation (num_actions x 1 array): the expected value of each arm
        - beta (float): inverse temperature
        - alpha (float): stickiness
        - curr_opto (boolean): whether the mouse is undergoing optogenetic stimulation
        - opto_change (string): what effect optogenetic stimulation has on the mouse parameters beta and alpha
            * "increase exploration": decrease inverse temperature
            * "decrease stickiness": decrease stickiness
            * "both": decrease both inverse temperature and stickiness
    """

    def __init__(self, num_arms, initial_state_prior, beta, alpha, opto_change):
        self.posterior = initial_state_prior
        self.expectation = np.zeros(num_arms)
        self.beta = beta
        self.alpha = alpha
        self.curr_opto = False
        self.opto_change = opto_change
    
    def choose_action(self, previous_action):
    
        """
        This function chooses an action based via softmax action selection.
        """

        stickiness = np.zeros(len(self.expectation))
        if previous_action is not None:
            stickiness[previous_action] += self.alpha
        
        exponent = np.exp(self.beta * (self.expectation + stickiness))
        probabilities = exponent / np.sum(exponent)
        return np.random.choice(range(len(exponent)), p=probabilities)
    
    def update_expectation(self, bandit_task):

        """
        This function calculates the expected value of each arm.
        """
        
        expectation_arm0 = bandit_task.arms_across_states[0][0]
        expectation_arm1 = self.posterior[0] * max(bandit_task.arms_across_states[0]) + self.posterior[1] * (1 - max(bandit_task.arms_across_states[0]))
        expectation_arm2 = self.posterior[0] * (1 - max(bandit_task.arms_across_states[0])) + self.posterior[1] * max(bandit_task.arms_across_states[0])
        self.expectation = np.asarray([expectation_arm0, expectation_arm1, expectation_arm2])
   
    def infer_state(self, bandit_task, action, reward):

        """
        This function implements recursive Bayesian filtering.

        Inputs: 
            - bandit_task (CorrelatedBanditTask object): 
            - action (int, either 0, 1, or 2):
            - reward (int, either 0 or 1): 
        """

        if action == 0:
            # Since the mouse chose the uncorrelated arm, we use a flat likelihood of 0.5
            likelihood = 0.5

            # But we still update the priors as we did previously
            updated_prior_high = p_no_switch * self.posterior[0] + p_switch * (1 - self.posterior[0])
            updated_prior_low = p_no_switch * self.posterior[0] + p_switch * (1 - self.posterior[0])

            # And calculate the new posteriors
            self.posterior[0] = likelihood * updated_prior_high
            self.posterior[1] = likelihood * updated_prior_low

            # And normalize the probabilities
            self.posterior = self.posterior/np.sum(self.posterior)
        else:
            # We start by calculating the probability that arm 1 is the high arm
            # given all of our observations

            # First, we calculate the likelihood, i.e., the probability of getting a 
            # reward after having picked an action given that arm 1 is the high arm
            if reward == 1 and action == 1:
                likelihood = max(bandit_task.arms_across_states[0])
            elif reward == 0 and action == 1:
                likelihood = 1 - max(bandit_task.arms_across_states[0])
            elif reward == 1 and action == 2:
                likelihood = 1 - max(bandit_task.arms_across_states[0])
            elif reward == 0 and action == 2:
                likelihood = max(bandit_task.arms_across_states[0])

            # Then, we calculate our prior, i.e., the probability that arm 1 is the high
            # arm given our previous history of observations
            p_no_switch = bandit_task.state_change_probabilities[0,0]
            p_switch = 1 - p_no_switch
            updated_prior_high = p_no_switch * self.posterior[0] + p_switch * (1 - self.posterior[0])

            # And calculate the posterior
            self.posterior[0] = likelihood * updated_prior_high

            # Then, we repeat the same process to calculate the probability that arm 1
            # is the low arm given all of our observations
            if reward == 1 and action == 1:
                likelihood = 1 - max(bandit_task.arms_across_states[0])
            elif reward == 0 and action == 1:
                likelihood = max(bandit_task.arms_across_states[0])
            elif reward == 1 and action == 2:
                likelihood = max(bandit_task.arms_across_states[0])
            elif reward == 0 and action == 2:
                likelihood = 1 - max(bandit_task.arms_across_states[0])
            updated_prior_low = p_no_switch * self.posterior[1] + p_switch * (1 - self.posterior[1])
            self.posterior[1] = likelihood * updated_prior_low

            # Normalize the probabilities
            self.posterior = self.posterior/np.sum(self.posterior)

class QLearningInferenceMouse():
    
    def __init__(self, num_arms, num_states, initial_state_prior, learning_rate, beta, alpha, opto_change):
        self.q_actions = np.zeros(num_arms)
        self.q_states = np.zeros(num_states)
        self.posterior = initial_state_prior

        self.learning_rate = learning_rate
        self.beta = beta
        self.alpha = alpha

        self.curr_opto = False
        self.opto_change = opto_change
    
    def choose_action(self, previous_action):

        stickiness = np.zeros(len(self.q_actions))
        if previous_action is not None:
            stickiness[previous_action] += self.alpha # the apparent value of 'previous action' is increased by alpha
        
        exponent = np.exp(self.beta * (self.q_actions + stickiness)) # raw action values + stickiness value
        probabilities = exponent / np.sum(exponent) # normalize
        return np.random.choice(range(len(exponent)), p=probabilities)
    
    def infer_state(self, bandit_task, action, reward):

        """
        This function implements recursive Bayesian filtering.

        Inputs: 
            - bandit_task (CorrelatedBanditTask object): 
            - action (int, either 0, 1, or 2):
            - reward (int, either 0 or 1): 
        """
        
        # Updating the prior is the same independently of our action
        p_no_switch = bandit_task.state_change_probabilities[0,0]
        p_switch = 1 - p_no_switch
        updated_prior_high = p_no_switch * self.posterior[0] + p_switch * (1 - self.posterior[0])
        updated_prior_low = p_no_switch * self.posterior[1] + p_switch * (1 - self.posterior[1])
        assert np.isclose(1.0, updated_prior_high + updated_prior_low) # these should sum to 1

        if action == 0:
            # Since the mouse chose the uncorrelated arm, we use a flat likelihood of 0.5
            likelihood_high = 0.5
            likelihood_low = 0.5

        else:
            # We picked a correlated arm and have a non-flat likelihood

            # First, we calculate the likelihood, i.e., the probability of getting a 
            # reward after having picked an action given that arm 1 is the high arm 
            if (reward == 1 and action == 1) or (reward == 0 and action == 2):
                # if we got reward at 1 or no reward at 2, there is high likelihood that arm 1 is the 'high reward'
                likelihood_high = max(bandit_task.arms_across_states[0])
            else:
                # there is low likelihood that arm 1 is the 'high reward'
                likelihood_high = 1 - max(bandit_task.arms_across_states[0])
                
            # since these arms are perfectly anticorrelated, we can do this -- update if we implement more complicated tasks
            likelihood_low = 1 - likelihood_high
            
        self.posterior[0] = likelihood_high * updated_prior_high # posterior for arm 1 being high reward (without normalization)
        self.posterior[1] = likelihood_low * updated_prior_low # posterior for arm 1 being low reward (without normalization)

        # And normalize the probabilities
        self.posterior = self.posterior/np.sum(self.posterior)

    def update_q_states(self, action, reward):
        # If the mouse chooses either of the correlated arms, we distribute the Q update according 
        # to the probability of being in each state
        # Q_a = sum_s p(a = s) Q(s)
        # \delta Q_s \propto d(-(Q_a - r)^2)/dQ_s = p(a = s) (r - Q_a)
        # if we took action i, update both abstract states according to how likely they are to be arm i
        if action == 1:
            self.q_states[0] += self.learning_rate * self.posterior[0] * (reward - self.q_actions[1]) # should use q_action instead of q_state I think
            self.q_states[1] += self.learning_rate * self.posterior[1] * (reward - self.q_actions[1]) # could update in parallel if you want
            
        # Since the posterior is defined in terms of arm 1, we switch the probabilities for the Q update for arm 2
        elif action == 2:
            self.q_states[0] += self.learning_rate * self.posterior[1] * (reward - self.q_actions[2]) # probability that '1 is low' is the same as '2 is high'
            self.q_states[1] += self.learning_rate * self.posterior[0] * (reward - self.q_actions[2]) # probability that '2 is low'
    
    def update_q_actions(self, action, reward):
        # If the mouse chooses the uncorrelated arm, update its Q value as you would normally
        if action == 0:
            self.q_actions[0] += self.learning_rate * (reward - self.q_actions[0])
        
        # whatever happened, we need to update our action values for actions (1,2) since these depend on our state inference which changes
        self.q_actions[1] = self.posterior[0] * self.q_states[0] + self.posterior[1] * self.q_states[1] # p(1 = high)*high + p(1 = low)*low
        self.q_actions[2] = self.posterior[0] * self.q_states[1] + self.posterior[1] * self.q_states[0] # p(1 = high)*_low_ + p(1 = low)*_high_

"""
TASKS
"""

class MultiArmedBanditTask():

    """
    This class implements a multi-armed bandit task, in which all the arms are independent of one another.
    """

    def __init__(self, arms, drift_rates, opto_probability, opto_trial_threshold, delta_beta, delta_alpha):
        self.arms = arms
        self.num_arms = len(arms)
        self.drift_rates = drift_rates
        self.opto_probability = opto_probability
        self.opto_trial_threshold = opto_trial_threshold
        self.delta_beta = delta_beta
        self.delta_alpha = delta_alpha
    
    def get_reward(self, arm):
        if random.random() < self.arms[arm]:
            return 1
        else: 
            return 0
    
    def drift_arms(self):
        for i in range(self.num_arms):
            noise = np.random.normal(0, self.drift_rates[i])
            self.arms[i] = self.arms[i] + noise
        self.arms = np.clip(self.arms, 0, 1)
    
    def begin_opto(self, agent):
        if random.random() < self.opto_probability:
            agent.curr_opto = True
            if agent.opto_change == "increase exploration":
                agent.beta *= self.delta_beta
            elif agent.opto_change == "decrease stickiness":
                agent.alpha -= self.delta_alpha
            elif agent.opto_change == "both":
                agent.beta *= self.delta_beta
                agent.alpha -= self.delta_alpha

    def end_opto(self, agent):
        agent.curr_opto = False
        if agent.opto_change == "increase exploration":
            agent.beta /= self.delta_beta
        elif agent.opto_change == "decrease stickiness":
            agent.alpha += self.delta_alpha
        elif agent.opto_change == "both":
            agent.beta /= self.delta_beta
            agent.alpha += self.delta_alpha
    
    def simulate(self, agent, num_trials):
        previous_action = None
        actions = []
        rewards = []
        switches = []

        for i in range(num_trials):
            current_action = agent.choose_action(previous_action)
            actions.append(current_action)
            if previous_action != current_action:
                switches_details = {
                    "trial_num": i,
                    "high_arm": np.argmax(self.arms),
                    "from": previous_action,
                    "to": current_action
                }
                switches.append(switches_details)
            
            reward = self.get_reward(current_action)
            rewards.append(reward)

            agent.update_q_values(current_action, reward)
            previous_action = current_action
        
        return actions, rewards, switches

class CorrelatedBanditTask(MultiArmedBanditTask):

    """
    This class implements a multi-armed bandit task, in which not all the arms are 
    """

    def __init__(self, arms_across_states, state_change_probabilities, opto_probability, opto_trial_threshold, delta_beta, delta_alpha):
        self.arms_across_states = arms_across_states
        self.state_change_probabilities = state_change_probabilities
        self.num_states = len(arms_across_states)

        self.curr_state = np.random.choice(self.num_states)
        self.curr_arms = arms_across_states[self.curr_state]
        self.num_arms = len(self.curr_arms)

        self.opto_probability = opto_probability
        self.opto_trial_threshold = opto_trial_threshold
        self.delta_beta = delta_beta
        self.delta_alpha = delta_alpha
    
    def get_reward(self, arm):
        if random.random() < self.curr_arms[arm]:
            return 1
        else: 
            return 0

    def update_states(self, trial_num):
        if random.random() < self.state_change_probabilities[self.curr_state, np.abs(self.curr_state - 1)]:
            prev_state = self.curr_state
            self.curr_state = np.abs(self.curr_state - 1)
            self.curr_arms = self.arms_across_states[self.curr_state]
            state_changes_details = {
                "trial_num": trial_num + 1,
                "from": prev_state,
                "to": self.curr_state
            }
            return state_changes_details
        else:
            return None
    
    def simulate(self, agent, num_trials):

        previous_action = None
        actions = []
        rewards = []
        switches = []
        state_changes = []

        posterior = np.zeros(shape=(num_trials, self.num_states))
        q_states = np.zeros(shape=(num_trials, self.num_states))
        q_actions = np.zeros(shape=(num_trials, self.num_arms))

        for i in range(num_trials):
            # The mouse picks the action
            current_action = agent.choose_action(previous_action)
            actions.append(current_action)

            # Check to see if the mouse switched its choice
            if previous_action != current_action:
                switches_details = {
                    "trial_num": i,
                    "high_arm": np.argmax(self.curr_arms),
                    "from": previous_action,
                    "to": current_action
                }
                switches.append(switches_details)
            
            # The mouse recieves a probabilisitic reward
            reward = self.get_reward(current_action)
            rewards.append(reward)

            # The mouse updates its estimate of the different states
            agent.update_q_states(current_action, reward)

            # Keep track of the abstract Q-values
            q_states[i,:] = agent.q_states

            # The mouse updates its estimate of the current state
            agent.infer_state(self, current_action, reward)

            # Keep track of state estimate
            posterior[i,:] = agent.posterior

            # The mouse updates its estimate of the different arms
            agent.update_q_actions(current_action, reward)

            # Keep track of the concrete Q-values
            q_actions[i,:] = agent.q_actions

            # Update the previous action and the state for the next trial
            previous_action = current_action
            state_change_details = self.update_states(i)
            if state_change_details is not None:
                state_changes.append(state_change_details)

        return actions, rewards, switches, state_changes, posterior, q_states, q_actions

    """
    def simulate(self, agent, num_trials):
        previous_action = None
        actions = []
        rewards = []
        switches = []
        state_changes = []
        opto_trials = []

        for i in range(num_trials):
            # The mouse picks the action
            current_action = agent.choose_action(previous_action)
            actions.append(current_action)
            if previous_action != current_action:
                switches_details = {
                    "trial_num": i,
                    "high_arm": np.argmax(self.curr_arms),
                    "from": previous_action,
                    "to": current_action
                }
                switches.append(switches_details)

            # The mouse receives a probabilistic reward
            reward = self.get_reward(current_action)
            rewards.append(reward)

            # If the mouse has underwent optogenetic stimulation, 
            # Keep track of the trial info immediately after the optogenetic stimulation
            # And then turn off the stimulation after the end of the trial
            if agent.curr_opto:
                opto_details = {
                    "trial_num": i,
                    "high_arm": np.argmax(self.curr_arms),
                    "previous_action": previous_action,
                    "current_action": current_action
                }
                opto_trials.append(opto_details)
                self.end_opto(agent)
            # If the mouse is both trained and chooses the high arm, then initiate optogenetic stimulation for the next trial
            if (num_trials > self.opto_trial_threshold) & (previous_action == np.argmax(self.curr_arms)):
                self.begin_opto(agent)

            # The mouse updates its estimates of the value of the different bandits
            agent.update_q_values(current_action, reward)

            # Update the previous action and the state for the next trial
            previous_action = current_action
            state_changes_details = self.update_states(i)
            if state_changes_details is not None:
                state_changes.append(state_changes_details)
        
        return actions, rewards, switches, state_changes, opto_trials
    """